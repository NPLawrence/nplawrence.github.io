---
<!-- layout: publication -->
title: "Almost Surely Stable Deep Dynamics"
collection: publications
type: "conference"
order: 6
year: 2020
authors: "Nathan P. Lawrence, Philip D. Loewen, Michael G. Forbes, Johan U. Backstrom and R. Bhushan Gopaluni"
journal: "In Proceedings of Advances in Neural Information Processing Systems 33"
journal_short: "NeurIPS"
notes: "NeurIPS Spotlight"
pdf: "2020C6_Lawrence_NeurIPS.pdf"
code: "https://github.com/NPLawrence/stochastic_dynamics"
link: "https://proceedings.neurips.cc//paper_files/paper/2020/hash/daecf755df5b1d637033bb29b319c39a-Abstract.html"
video: "https://nips.cc/virtual/2020/protected/poster_daecf755df5b1d637033bb29b319c39a.html"
poster: "https://figshare.com/articles/poster/Almost_Surely_Stable_Deep_Dynamics/13238408"
thumbnail: "lawrence_neurips_2020.png"
arxiv: "https://arxiv.org/abs/2103.14722"
description: "We introduce a method for learning provably stable deep neural network based dynamic models from observed data. Specifically, we consider discrete-time stochastic dynamic models, as they are of particular interest in practical applications such as estimation and control. However, these aspects exacerbate the challenge of guaranteeing stability. Our method works by embedding a Lyapunov neural network into the dynamic model, thereby inherently satisfying the stability criterion. To this end, we propose two approaches and apply them in both the deterministic and stochastic settings: one exploits convexity of the Lyapunov function, while the other enforces stability through an implicit output layer. We demonstrate the utility of each approach through numerical examples."
---
